{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import preprocess_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open('../fake_data/interviews.txt', 'r') as file:\n",
    "    text = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "processed_text = preprocess_string(text)\n",
    "\n",
    "# Create a dictionary from the preprocessed text\n",
    "dictionary = corpora.Dictionary([processed_text])\n",
    "\n",
    "# Create a corpus\n",
    "corpus = [dictionary.doc2bow(processed_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5)\n",
    "\n",
    "# Get the topics\n",
    "topics = lda_model.print_topics(num_topics=5, num_words=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.051*\"tag\" + 0.043*\"crime\" + 0.035*\"electron\" + 0.033*\"commit\" + 0.028*\"offend\" + 0.023*\"monitor\" + 0.022*\"question\" + 0.018*\"answer\" + 0.018*\"us\" + 0.017*\"think\"')\n",
      "(1, '0.049*\"crime\" + 0.045*\"tag\" + 0.031*\"electron\" + 0.028*\"commit\" + 0.023*\"offend\" + 0.020*\"us\" + 0.018*\"question\" + 0.017*\"answer\" + 0.017*\"wear\" + 0.016*\"reduc\"')\n",
      "(2, '0.048*\"crime\" + 0.040*\"tag\" + 0.028*\"electron\" + 0.024*\"commit\" + 0.022*\"us\" + 0.019*\"monitor\" + 0.017*\"answer\" + 0.017*\"reduc\" + 0.016*\"offend\" + 0.014*\"time\"')\n",
      "(3, '0.040*\"tag\" + 0.036*\"crime\" + 0.025*\"us\" + 0.023*\"commit\" + 0.022*\"electron\" + 0.018*\"question\" + 0.018*\"offend\" + 0.015*\"wear\" + 0.015*\"reduc\" + 0.014*\"think\"')\n",
      "(4, '0.052*\"tag\" + 0.032*\"crime\" + 0.031*\"electron\" + 0.028*\"commit\" + 0.021*\"offend\" + 0.019*\"us\" + 0.017*\"question\" + 0.017*\"reduc\" + 0.015*\"time\" + 0.014*\"monitor\"')\n"
     ]
    }
   ],
   "source": [
    "# Print the topics\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the topics don't look terribly useful really, likely a quality of the poor fake interview data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some sentiment polarity in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment polarity\n",
    "sentiment = TextBlob(text).sentiment.polarity\n",
    "#The sentiment polarity is a numerical value ranging from -1 to 1, \n",
    "#where values closer to -1 indicate negative sentiment, values closer to 1 \n",
    "#indicate positive sentiment, and values around 0 indicate neutrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 6.86%\n",
      "Neutral: 93.14%\n",
      "Negative: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Assign sentiment labels\n",
    "if sentiment > 0:\n",
    "    sentiment_label = 'positive'\n",
    "elif sentiment < 0:\n",
    "    sentiment_label = 'negative'\n",
    "else:\n",
    "    sentiment_label = 'neutral'\n",
    "\n",
    "# Calculate sentiment proportion\n",
    "positive_proportion = max(sentiment, 0)\n",
    "\n",
    "# calculate the negative proportion of sentiment\n",
    "#min(sentiment, 0): This expression compares the sentiment polarity (sentiment)\n",
    "#with 0 and returns the smaller of the two values. It ensures that only negative \n",
    "#polarity values are considered when calculating the negative sentiment proportion. \n",
    "#If the sentiment polarity is positive or neutral (i.e., greater than or equal to 0), the expression evaluates to 0.\n",
    "#-min(sentiment, 0): The negative value of the smaller of sentiment and 0 is obtained by negating the result. \n",
    "#This ensures that the negative proportion of sentiment is always a positive value.\n",
    "\n",
    "negative_proportion = -min(sentiment, 0)\n",
    "neutral_proportion = 1 - positive_proportion - negative_proportion\n",
    "\n",
    "# Print sentiment proportions\n",
    "print(f\"Positive: {positive_proportion*100:.2f}%\")\n",
    "print(f\"Neutral: {neutral_proportion*100:.2f}%\")\n",
    "print(f\"Negative: {negative_proportion*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again the above clearly looks like it needs refinement too. There was some negativity in the interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
